{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Seed = 8\n",
      "Episode 25\tTime 0.48sec(m)\tAvg. Score both 100ep: -0.0006\tMax: 0.10000\n",
      "Episode 50\tTime 0.95sec(m)\tAvg. Score both 100ep: -0.0002\tMax: 0.10000\n",
      "Episode 75\tTime 1.06sec(m)\tAvg. Score both 100ep: 0.0003\tMax: 0.100010\n",
      "Episode 100\tTime 1.09sec(m)\tAvg. Score both 100ep: 0.0000\tMax: 0.100010\n",
      "Episode 125\tTime 1.11sec(m)\tAvg. Score both 100ep: -0.0003\tMax: 0.10000\n",
      "Episode 150\tTime 1.14sec(m)\tAvg. Score both 100ep: -0.0008\tMax: 0.10000\n",
      "Episode 175\tTime 1.15sec(m)\tAvg. Score both 100ep: -0.0017\tMax: 0.10000\n",
      "Episode 200\tTime 1.16sec(m)\tAvg. Score both 100ep: -0.0022\tMax: 0.10000\n",
      "Episode 225\tTime 1.16sec(m)\tAvg. Score both 100ep: -0.0024\tMax: 0.10000\n",
      "Episode 250\tTime 1.18sec(m)\tAvg. Score both 100ep: -0.0025\tMax: 0.10000\n",
      "Episode 275\tTime 1.21sec(m)\tAvg. Score both 100ep: -0.0024\tMax: 0.10000\n",
      "Episode 300\tTime 1.22sec(m)\tAvg. Score both 100ep: -0.0020\tMax: 0.10000\n",
      "Episode 325\tTime 1.22sec(m)\tAvg. Score both 100ep: -0.0015\tMax: 0.10000\n",
      "Episode 350\tTime 1.27sec(m)\tAvg. Score both 100ep: -0.0008\tMax: 0.10000\n",
      "Episode 375\tTime 1.32sec(m)\tAvg. Score both 100ep: 0.0005\tMax: 0.100010\n",
      "Episode 400\tTime 1.38sec(m)\tAvg. Score both 100ep: 0.0021\tMax: 0.100000\n",
      "Episode 425\tTime 1.44sec(m)\tAvg. Score both 100ep: 0.0045\tMax: 0.100000\n",
      "Episode 450\tTime 1.51sec(m)\tAvg. Score both 100ep: 0.0069\tMax: 0.100000\n",
      "Episode 475\tTime 1.56sec(m)\tAvg. Score both 100ep: 0.0091\tMax: 0.100010\n",
      "Episode 500\tTime 1.59sec(m)\tAvg. Score both 100ep: 0.0109\tMax: 0.100000\n",
      "Episode 525\tTime 1.65sec(m)\tAvg. Score both 100ep: 0.0121\tMax: 0.10000\n",
      "Episode 550\tTime 1.71sec(m)\tAvg. Score both 100ep: 0.0132\tMax: 0.20000\n",
      "Episode 575\tTime 1.80sec(m)\tAvg. Score both 100ep: 0.0147\tMax: 0.30000\n",
      "Episode 600\tTime 1.86sec(m)\tAvg. Score both 100ep: 0.0168\tMax: 0.300000\n",
      "Episode 625\tTime 1.93sec(m)\tAvg. Score both 100ep: 0.0191\tMax: 0.300000\n",
      "Episode 650\tTime 2.08sec(m)\tAvg. Score both 100ep: 0.0218\tMax: 0.60000\n",
      "Episode 675\tTime 2.18sec(m)\tAvg. Score both 100ep: 0.0250\tMax: 0.60000\n",
      "Episode 700\tTime 2.56sec(m)\tAvg. Score both 100ep: 0.0293\tMax: 2.20000\n",
      "Episode 725\tTime 2.67sec(m)\tAvg. Score both 100ep: 0.0348\tMax: 2.20000\n",
      "Episode 750\tTime 2.76sec(m)\tAvg. Score both 100ep: 0.0404\tMax: 2.20000\n",
      "Episode 775\tTime 2.83sec(m)\tAvg. Score both 100ep: 0.0454\tMax: 2.20000\n",
      "Episode 800\tTime 2.87sec(m)\tAvg. Score both 100ep: 0.0489\tMax: 2.20000\n",
      "Episode 825\tTime 2.91sec(m)\tAvg. Score both 100ep: 0.0510\tMax: 2.20000\n",
      "Episode 850\tTime 2.95sec(m)\tAvg. Score both 100ep: 0.0527\tMax: 2.20000\n",
      "Episode 875\tTime 3.24sec(m)\tAvg. Score both 100ep: 0.0547\tMax: 2.60000\n",
      "Episode 900\tTime 3.27sec(m)\tAvg. Score both 100ep: 0.0577\tMax: 2.60000\n",
      "Episode 925\tTime 3.33sec(m)\tAvg. Score both 100ep: 0.0607\tMax: 2.60000\n",
      "Episode 950\tTime 3.50sec(m)\tAvg. Score both 100ep: 0.0639\tMax: 2.60000\n",
      "Episode 975\tTime 3.65sec(m)\tAvg. Score both 100ep: 0.0676\tMax: 2.60000\n",
      "Episode 1000\tTime 3.80sec(m)\tAvg. Score both 100ep: 0.0710\tMax: 2.60000\n",
      "Episode 1025\tTime 3.96sec(m)\tAvg. Score both 100ep: 0.0754\tMax: 2.60000\n",
      "Episode 1050\tTime 3.99sec(m)\tAvg. Score both 100ep: 0.0798\tMax: 2.60000\n",
      "Episode 1075\tTime 4.08sec(m)\tAvg. Score both 100ep: 0.0834\tMax: 2.60000\n",
      "Episode 1100\tTime 4.16sec(m)\tAvg. Score both 100ep: 0.0864\tMax: 2.60000\n",
      "Episode 1125\tTime 4.23sec(m)\tAvg. Score both 100ep: 0.0886\tMax: 2.60000\n",
      "Episode 1150\tTime 4.30sec(m)\tAvg. Score both 100ep: 0.0905\tMax: 2.600000\n",
      "Episode 1175\tTime 4.47sec(m)\tAvg. Score both 100ep: 0.0931\tMax: 2.60000\n",
      "Episode 1200\tTime 4.87sec(m)\tAvg. Score both 100ep: 0.0972\tMax: 2.60000\n",
      "Episode 1225\tTime 5.19sec(m)\tAvg. Score both 100ep: 0.1035\tMax: 2.60000\n",
      "Episode 1250\tTime 5.37sec(m)\tAvg. Score both 100ep: 0.1113\tMax: 2.60000\n",
      "Episode 1275\tTime 5.95sec(m)\tAvg. Score both 100ep: 0.1206\tMax: 2.60000\n",
      "Episode 1300\tTime 6.75sec(m)\tAvg. Score both 100ep: 0.1324\tMax: 2.700000\n",
      "Episode 1325\tTime 7.97sec(m)\tAvg. Score both 100ep: 0.1481\tMax: 2.700000\n",
      "Episode 1334\tTime 115.94sec\tAverage Score: 2.6000\tMax: 2.6000\tMin: 2.600Min Epsilon reached\n",
      "Min Epsilon reached\n",
      "Episode 1350\tTime 9.32sec(m)\tAvg. Score both 100ep: 0.1693\tMax: 2.700000\n",
      "Episode 1375\tTime 10.49sec(m)\tAvg. Score both 100ep: 0.1950\tMax: 2.70000\n",
      "Episode 1400\tTime 11.69sec(m)\tAvg. Score both 100ep: 0.2218\tMax: 2.70000\n",
      "Episode 1425\tTime 12.85sec(m)\tAvg. Score both 100ep: 0.2474\tMax: 2.70000\n",
      "Episode 1450\tTime 13.84sec(m)\tAvg. Score both 100ep: 0.2707\tMax: 2.70000\n",
      "Episode 1475\tTime 14.62sec(m)\tAvg. Score both 100ep: 0.2905\tMax: 2.70000\n",
      "Episode 1500\tTime 16.82sec(m)\tAvg. Score both 100ep: 0.3098\tMax: 2.70000\n",
      "Episode 1525\tTime 18.94sec(m)\tAvg. Score both 100ep: 0.3306\tMax: 2.70000\n",
      "Episode 1550\tTime 20.81sec(m)\tAvg. Score both 100ep: 0.3523\tMax: 2.70000\n",
      "Episode 1575\tTime 21.98sec(m)\tAvg. Score both 100ep: 0.3756\tMax: 2.70000\n",
      "Episode 1600\tTime 23.02sec(m)\tAvg. Score both 100ep: 0.3981\tMax: 2.70000\n",
      "Episode 1625\tTime 23.52sec(m)\tAvg. Score both 100ep: 0.4159\tMax: 2.70000\n",
      "Episode 1650\tTime 24.20sec(m)\tAvg. Score both 100ep: 0.4291\tMax: 2.70000\n",
      "Episode 1675\tTime 25.75sec(m)\tAvg. Score both 100ep: 0.4409\tMax: 2.70000\n",
      "Episode 1700\tTime 27.14sec(m)\tAvg. Score both 100ep: 0.4533\tMax: 2.70000\n",
      "Episode 1725\tTime 28.64sec(m)\tAvg. Score both 100ep: 0.4678\tMax: 2.70000\n",
      "Episode 1750\tTime 30.20sec(m)\tAvg. Score both 100ep: 0.4855\tMax: 2.70000\n",
      "Episode 1775\tTime 31.88sec(m)\tAvg. Score both 100ep: 0.5042\tMax: 2.70000\n",
      "Episode 1800\tTime 33.37sec(m)\tAvg. Score both 100ep: 0.5216\tMax: 2.70000\n",
      "Episode 1821\tTime 223.49sec\tAverage Score: 2.6000\tMax: 2.6000\tMin: 2.600"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from ddpg_agent import Agent\n",
    "from collections import deque\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#from ddpg_agent import InitSharedMemory\n",
    "import os, sys\n",
    "\n",
    "seed= 8\n",
    "print(\"Seed = {}\".format(seed))\n",
    "Agent1 = Agent(state_size, action_size, seed) \n",
    "Agent2 = Agent(state_size, action_size, seed) \n",
    "\n",
    "def train(n_episodes=500, max_t=2000, print_every=25): #max timesteps of 1000 was reached once\n",
    "        scores_deque = deque(maxlen=print_every)\n",
    "        #InitSharedMemory(action_size, seed)\n",
    "        scores_A1 = [] #list of mean scores for each episode\n",
    "        scores_A2 = [] #list of mean scores for each episode\n",
    "        scores_both=[]\n",
    "        scores_max = []        \n",
    "        scores_min = []    \n",
    "        duration_means=[]    \n",
    "\n",
    "        for i_episode in range(1, n_episodes+1):\n",
    "            # each episode --> train agents in parallel\n",
    "            start_time=time.time()\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            states = env_info.vector_observations\n",
    "            scores = np.zeros(num_agents)\n",
    "    \n",
    "            Agent1.reset_noise()   \n",
    "            Agent2.reset_noise()   \n",
    "            timestep=0\n",
    "\n",
    "            #for t in range(max_t) (maximum number of timesteps in an episode) --> not used\n",
    "            while timestep < max_t:\n",
    "\n",
    "                A1_actions = Agent1.act(states[0], add_noise=True)         \n",
    "                A2_actions = Agent2.act(states[1], add_noise=True)    \n",
    "                actions=[A1_actions, A2_actions]\n",
    "                # execute actions parallel\n",
    "                #actions = np.clip(actions, -1, 1,dtype=float) # check\n",
    "                env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "                next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "                rewards = env_info.rewards                         # get reward (for each agent)\n",
    "                scores += env_info.rewards                         # update the score (for each agent)\n",
    "                dones = env_info.local_done                        # see if episode finished\n",
    "                #if True in ( obs !='' for obs in env_info.text_observations):\n",
    "                if (any(ob !='' for ob in env_info.text_observations )):\n",
    "                    print(env_info.text_observations)\n",
    "                \n",
    "                #shared_rewards=rewards[0]+rewards[1] # not that successful IME\n",
    "                shared_rewards=max(rewards[0],rewards[1])\n",
    "                # do save experience and take some time to learn\n",
    "                for t in range(num_agents):\n",
    "                    Agent1.step(states[t], actions[t], shared_rewards, next_states[t], dones[t], timestep)\n",
    "                    Agent2.step(states[t], actions[t], shared_rewards, next_states[t], dones[t], timestep)\n",
    "                # end of explicit learning functions\n",
    "                states = next_states                               # roll over states to next time step\n",
    "                timestep+=1\n",
    "                if np.any(dones):                                  # exit loop if episode finished\n",
    "                    break \n",
    "                if (timestep==max_t): \n",
    "                    print(\" Max timesteps {} reached\".format(timestep))\n",
    "\n",
    "            scores_max.append(np.max(scores))             # save highest score for a single agent         \n",
    "            scores_min.append(np.min(scores))\n",
    "            scores_deque.append(np.mean(scores))\n",
    "            scores_A1.append(np.mean(scores[0]))\n",
    "            scores_A2.append(np.mean(scores[1]))\n",
    "            scores_both.append(max(np.mean(scores_A1), np.mean(scores_A2)) )\n",
    "            duration = time.time() - start_time\n",
    "            duration_means.append(duration)\n",
    "\n",
    "            print('\\rEpisode {}\\tTime {:.2f}sec\\tAverage Score: {:.4f}\\tMax: {:.4f}\\tMin: {:.3f}'.format(i_episode,duration, np.mean(scores), scores_max[i_episode-1], scores_min[i_episode-1]), end=\"\") \n",
    "            #  scores_deque\n",
    "            if i_episode % print_every == 0:\n",
    "                torch.save(Agent1.actor_local.state_dict(), f'checkpoint_actor_{i_episode}.pth')\n",
    "                torch.save(Agent2.actor_local.state_dict(), f'checkpoint_actor_{i_episode}.pth')\n",
    "                torch.save(Agent1.critic_local.state_dict(), f'checkpoint_critic_{i_episode}.pth')\n",
    "                torch.save(Agent2.critic_local.state_dict(), f'checkpoint_critic_{i_episode}.pth')\n",
    "                #print('\\rEpisode {}\\tAverage Score: {:.2f}\\tlast Episode Score: {:.2f}'.format(i_episode, np.mean(scores_deque), np.mean(scores)))\n",
    "                print('\\rEpisode {}\\tTime {:.2f}sec(m)\\tAvg. Score both 100ep: {:.4f}\\tMax: {:.4f}'.format(i_episode,np.mean(duration_means), np.mean(scores_both[-100:]), np.max(scores_max)))   \n",
    "                         \n",
    "            # Specifically, After each episode, we add up the rewards that each agent received, to get a score for each agent. This yields 2 (potentially different) scores. \n",
    "            # We then take the maximum of these 2 scores. This yields a single score for each episode.            \n",
    "            # #The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5.\n",
    "\n",
    "            # if timestep>10000: #and ScoresOver30(scores_means):\n",
    "            if i_episode>1000:\n",
    "                scores_over_last_100_episodes= scores_both[-100:]               \n",
    "                # print(\"\\tMean over last 100 Episodes = {:.3f}\".format(np.mean(scores_over_last_100_episodes)))\n",
    "                # restart programm when\n",
    "                # if (np.mean(scores_over_last_100_episodes) < 0.01):\n",
    "                #     print(\"Restarted\")\n",
    "                #     os.execv(sys.argv[0], sys.argv)\n",
    "            \n",
    "                if( (np.array(scores_over_last_100_episodes) > 0.5).all() ):\n",
    "                    torch.save(Agent1.actor_local.state_dict(), f'checkpoint_actor_solved.pth')\n",
    "                    torch.save(Agent2.actor_local.state_dict(), f'checkpoint_actor_solved.pth')\n",
    "                    torch.save(Agent1.critic_local.state_dict(), f'checkpoint_critic_solved.pth')\n",
    "                    torch.save(Agent2.critic_local.state_dict(), f'checkpoint_critic_solved.pth')\n",
    "                    return scores_both\n",
    "\n",
    "        print('Score (max over agents) from episode {}: {}'.format(i_episode, np.max(scores)))\n",
    "        return scores_both\n",
    "\n",
    "scores = train(2000) # 2000 episodes should be enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
